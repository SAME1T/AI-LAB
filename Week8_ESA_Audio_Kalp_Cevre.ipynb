{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adbe461",
   "metadata": {},
   "source": [
    "# 8. Hafta – ESA (3B CNN) ile Ses Sınıflandırma\n",
    "\n",
    "**Not defteri adı:** `Week8_ESA_Audio_Kalp_Cevre.ipynb`  \n",
    "**Klasör:** `C:\\Users\\The Coder Farmer\\Desktop\\AI LAB`\n",
    "\n",
    "Bu dosya 3 bölümden oluşur:\n",
    "- **Soru 1 (10p):** Kendi ESA (Conv3D) modelini tanımla.\n",
    "- **Soru 2 (45p):** **Kalp** verisiyle eğitim — accuracy, F1, loss grafikleri + örnek tahmin.\n",
    "- **Soru 3 (45p):** **Çevre** verisiyle eğitim — accuracy, F1, loss grafikleri + **confusion matrix**.\n",
    "\n",
    "> Veri setini indirip köke veya sınıf klasörleriyle yerleştir. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install librosa soundfile tensorflow scikit-learn matplotlib numpy --quiet\n",
    "import os, glob, re, random\n",
    "import numpy as np\n",
    "import librosa, soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "BASE = r\"C:\\Users\\The Coder Farmer\\Desktop\\AI LAB\"\n",
    "\n",
    "# GPU bellek büyümesini aç (varsa)\n",
    "for g in tf.config.list_physical_devices('GPU'):\n",
    "    try: tf.config.experimental.set_memory_growth(g, True)\n",
    "    except: pass\n",
    "\n",
    "ALLOW = {\".wav\",\".mp3\",\".flac\",\".ogg\",\".m4a\"}\n",
    "SR, N_MELS, HOP, N_FFT = 22050, 64, 512, 1024\n",
    "DEPTH, SLICE_W, BLOCK_HOP = 8, 8, 4\n",
    "\n",
    "def label_from_name(fname):\n",
    "    b = os.path.splitext(os.path.basename(fname))[0]\n",
    "    m = re.match(r\"^([A-Za-z]+)\", b)\n",
    "    return (m.group(1) if m else 'unknown').lower()\n",
    "\n",
    "def list_audio_with_labels(root):\n",
    "    sub = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root,d))]\n",
    "    items = []\n",
    "    cls_dirs = [d for d in sub if any(os.path.splitext(p)[1].lower() in ALLOW for p in glob.glob(os.path.join(root,d,'*')))]\n",
    "    if cls_dirs:   # class folders\n",
    "        for c in cls_dirs:\n",
    "            for p in glob.glob(os.path.join(root,c,'*')):\n",
    "                if os.path.splitext(p)[1].lower() in ALLOW: items.append((p,c.lower()))\n",
    "    else:          # flat folder\n",
    "        for p in glob.glob(os.path.join(root,'*')):\n",
    "            if os.path.splitext(p)[1].lower() in ALLOW: items.append((p,label_from_name(p)))\n",
    "    return items\n",
    "\n",
    "def melspec01(y, sr=SR):\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP, n_mels=N_MELS, power=2.0)\n",
    "    S = librosa.power_to_db(S, ref=np.max)\n",
    "    mn, mx = S.min(), S.max();  return (S - mn) / (mx - mn + 1e-8)\n",
    "\n",
    "def spec_to_blocks(S01, depth=DEPTH, w=SLICE_W, hop=BLOCK_HOP):\n",
    "    M, T = S01.shape; need = depth*w; out=[]\n",
    "    for st in range(0, T-need+1, hop):\n",
    "        stack = [S01[:, st+d*w: st+(d+1)*w] for d in range(depth)]\n",
    "        out.append(np.stack(stack,0)[...,None].astype(np.float32))\n",
    "    return np.asarray(out) if out else np.zeros((0, depth, M, w, 1), np.float32)\n",
    "\n",
    "def load_dataset(root, max_blocks_per_file=30):\n",
    "    pairs = list_audio_with_labels(root); assert pairs, f\"Ses yok: {root}\"\n",
    "    labels = sorted({lab for _,lab in pairs}); lab2id = {l:i for i,l in enumerate(labels)}\n",
    "    Xs, ys, files = [], [], []\n",
    "    for p,lab in pairs:\n",
    "        try:\n",
    "            y,_ = librosa.load(p, sr=SR, mono=True)\n",
    "            blks = spec_to_blocks(melspec01(y))\n",
    "            if len(blks)==0: continue\n",
    "            if max_blocks_per_file and len(blks)>max_blocks_per_file:\n",
    "                idx = np.random.choice(len(blks), max_blocks_per_file, replace=False); blks = blks[idx]\n",
    "            Xs.append(blks); ys.append(np.full((len(blks),), lab2id[lab], np.int64)); files += [p]*len(blks)\n",
    "        except Exception as e:\n",
    "            print(\"Skip:\", p, e)\n",
    "    return np.concatenate(Xs,0), np.concatenate(ys,0), labels, files\n",
    "\n",
    "def onehot(y, n): oh=np.zeros((len(y),n),np.float32); oh[np.arange(len(y)),y]=1; return oh\n",
    "\n",
    "def plot_curves(h, tag):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "    ax[0].plot(h.history['accuracy']); ax[0].plot(h.history['val_accuracy']); ax[0].set_title(f'{tag} Accuracy'); ax[0].legend(['train','val']); ax[0].grid(True)\n",
    "    ax[1].plot(h.history['loss']);     ax[1].plot(h.history['val_loss']);     ax[1].set_title(f'{tag} Loss');     ax[1].legend(['train','val']); ax[1].grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b69e49",
   "metadata": {},
   "source": [
    "## Soru 1 — Kendi ESA modelini oluştur (10p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_esa(input_shape, n_classes):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv3D(16, 3, padding='same', activation='relu')(inp)\n",
    "    x = layers.MaxPool3D((1,2,2))(x)\n",
    "    x = layers.Conv3D(32, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPool3D((2,2,2))(x)\n",
    "    x = layers.Conv3D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
    "    model = models.Model(inp, out, name='ESA_MyModel')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"ESA modeli hazır.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b09cd",
   "metadata": {},
   "source": [
    "## Soru 2 — Kalp verisiyle ESA eğitimi (accuracy, F1, loss + tahmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749259e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HEART_ROOT = os.path.join(BASE, \"heart\")  # Düz klasör veya class klasörleri\n",
    "if not os.path.exists(HEART_ROOT): HEART_ROOT = BASE  # kökteyse dosya adına göre etiketlenecek\n",
    "\n",
    "Xh, yh, ch, fh = load_dataset(HEART_ROOT, max_blocks_per_file=30)\n",
    "idx = np.arange(len(yh))\n",
    "Xtmp, Xt, ytmp, yt, itmp, it = train_test_split(Xh, yh, idx, test_size=0.2, stratify=yh, random_state=SEED)\n",
    "Xtr, Xv, ytr, yv, itr, iv = train_test_split(Xtmp, ytmp, itmp, test_size=0.2, stratify=ytmp, random_state=SEED)\n",
    "\n",
    "m = build_esa(Xtr.shape[1:], len(ch))\n",
    "h = m.fit(Xtr, onehot(ytr,len(ch)), validation_data=(Xv, onehot(yv,len(ch))), epochs=20, batch_size=32, verbose=1)\n",
    "plot_curves(h, \"HEART\")\n",
    "\n",
    "yp = np.argmax(m.predict(Xt, verbose=0),1)\n",
    "print(f\"[HEART] Test Acc={accuracy_score(yt,yp):.4f} | F1(macro)={f1_score(yt,yp,average='macro'):.4f}\")\n",
    "print(classification_report(yt, yp, target_names=ch))\n",
    "\n",
    "# Örnek tahminler\n",
    "for j in np.random.choice(len(yt), size=min(6,len(yt)), replace=False):\n",
    "    print(os.path.basename(fh[it]), \"-> pred:\", ch[yp[j]], \"| true:\", ch[yt[j]]); break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9eb654",
   "metadata": {},
   "source": [
    "## Soru 3 — Çevre verisiyle ESA eğitimi (accuracy, F1, loss + confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ENV_ROOT = os.path.join(BASE, \"environment\")\n",
    "if not os.path.exists(ENV_ROOT): ENV_ROOT = BASE\n",
    "\n",
    "Xe, ye, ce, fe = load_dataset(ENV_ROOT, max_blocks_per_file=30)\n",
    "idx = np.arange(len(ye))\n",
    "Xtmp, Xt, ytmp, yt, itmp, it = train_test_split(Xe, ye, idx, test_size=0.2, stratify=ye, random_state=SEED)\n",
    "Xtr, Xv, ytr, yv, itr, iv = train_test_split(Xtmp, ytmp, itmp, test_size=0.2, stratify=ytmp, random_state=SEED)\n",
    "\n",
    "m2 = build_esa(Xtr.shape[1:], len(ce))\n",
    "h2 = m2.fit(Xtr, onehot(ytr,len(ce)), validation_data=(Xv, onehot(yv,len(ce))), epochs=20, batch_size=32, verbose=1)\n",
    "plot_curves(h2, \"ENV\")\n",
    "\n",
    "yp = np.argmax(m2.predict(Xt, verbose=0),1)\n",
    "print(f\"[ENV] Test Acc={accuracy_score(yt,yp):.4f} | F1(macro)={f1_score(yt,yp,average='macro'):.4f}\")\n",
    "print(classification_report(yt, yp, target_names=ce))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(yt, yp, labels=range(len(ce)))\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "im = ax.imshow(cm); ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(len(ce)), yticks=np.arange(len(ce)),\n",
    "       xticklabels=ce, yticklabels=ce, xlabel='Pred', ylabel='True', title='Environment – Confusion Matrix')\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, ha='right'); \n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, cm[i,j], ha='center', va='center')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv_310)",
   "language": "python",
   "name": "venv_310"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
